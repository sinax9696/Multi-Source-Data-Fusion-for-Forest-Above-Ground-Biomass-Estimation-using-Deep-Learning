{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:18:25.182322: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 22:18:25.204960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 22:18:25.204987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 22:18:25.205610: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 22:18:25.209765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 22:18:25.609150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from keras_unet_collection import models, base, utils\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from skimage import io  \n",
    "from keras import backend as K\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available patches: 1681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Patches: 100%|██████████| 400/400 [00:01<00:00, 279.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shapes:\n",
      "x_train: (256, 128, 128, 31)\n",
      "y_train: (256, 128, 128, 1)\n",
      "x_val: (64, 128, 128, 31)\n",
      "y_val: (64, 128, 128, 1)\n",
      "x_test: (80, 128, 128, 31)\n",
      "y_test: (80, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_remote_sensing_data(desired_num_patches, random_seed):\n",
    "    \"\"\"\n",
    "    Load and preprocess remote sensing data with random patch selection.\n",
    "    \n",
    "    Args:\n",
    "        desired_num_patches (int): Number of patches to randomly select\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Your data directories\n",
    "    optic_images_dir = 'Sentinel 2_Patches'\n",
    "    sar_images_dir = 'Sentinel1_Patches'\n",
    "    agb_maps_dir = 'AGB_GroundTruth_Patches'\n",
    "    fvs_images_dir = 'Climatic_Parameters_Patches'\n",
    "    slope_images_dir = 'UNET_Slope_Patches/Patches'\n",
    "    aspect_images_dir = 'Topography_Aspect_Patches'\n",
    "    landsat8_patches_dir = 'Topography_Slope_Patches'\n",
    "\n",
    "    # Get the list of file names in the directories\n",
    "    agb_map_files = sorted(os.listdir(agb_maps_dir))\n",
    "    optic_image_files = sorted(os.listdir(optic_images_dir))\n",
    "    sar_image_files = sorted(os.listdir(sar_images_dir))\n",
    "    fvs_image_files = sorted(os.listdir(fvs_images_dir))\n",
    "    slope_image_files = sorted(os.listdir(slope_images_dir))\n",
    "    aspect_image_files = sorted(os.listdir(aspect_images_dir))\n",
    "    landsat8_image_files = sorted(os.listdir(landsat8_patches_dir))\n",
    "\n",
    "    # Verify all directories have the same number of files\n",
    "    total_patches = len(agb_map_files)\n",
    "    assert all(len(files) == total_patches for files in [\n",
    "        optic_image_files, sar_image_files, fvs_image_files,\n",
    "        slope_image_files, aspect_image_files, landsat8_image_files\n",
    "    ]), \"All directories must have the same number of files\"\n",
    "\n",
    "    print(f\"Total available patches: {total_patches}\")\n",
    "    \n",
    "    # Randomly select indices\n",
    "    selected_indices = np.random.choice(\n",
    "        total_patches, \n",
    "        size=min(desired_num_patches, total_patches), \n",
    "        replace=False\n",
    "    )\n",
    "    selected_indices = sorted(selected_indices)  # Sort for consistent file loading\n",
    "    \n",
    "    # Initialize empty lists to store data\n",
    "    x_train_list = []\n",
    "    agb_values_list = []\n",
    "\n",
    "    # Loop through selected patches\n",
    "    for idx in tqdm(selected_indices, desc='Loading Patches'):\n",
    "        try:\n",
    "            # Load images\n",
    "            optic_image = io.imread(os.path.join(optic_images_dir, optic_image_files[idx]))\n",
    "            sar_image = io.imread(os.path.join(sar_images_dir, sar_image_files[idx]))\n",
    "            agb_map = io.imread(os.path.join(agb_maps_dir, agb_map_files[idx]))\n",
    "            fvs_image = io.imread(os.path.join(fvs_images_dir, fvs_image_files[idx]))\n",
    "            slope_image = io.imread(os.path.join(slope_images_dir, slope_image_files[idx]))\n",
    "            aspect_image = io.imread(os.path.join(aspect_images_dir, aspect_image_files[idx]))\n",
    "            landsat8_image = io.imread(os.path.join(landsat8_patches_dir, landsat8_image_files[idx]))\n",
    "\n",
    "            # Verify Landsat 8 bands\n",
    "            if landsat8_image.shape[-1] != 7:\n",
    "                print(f\"Skipping patch {idx}: Invalid Landsat 8 bands {landsat8_image.shape[-1]}\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all datasets using min-max scaling\n",
    "            def normalize(img):\n",
    "                return (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)\n",
    "\n",
    "            optic_image_normalized = normalize(optic_image)\n",
    "            sar_image_normalized = normalize(sar_image)\n",
    "            fvs_image_normalized = normalize(fvs_image)\n",
    "            slope_image_normalized = normalize(slope_image)\n",
    "            aspect_image_normalized = normalize(aspect_image)\n",
    "            landsat8_image_normalized = normalize(landsat8_image)\n",
    "\n",
    "            # Expand dimensions for single-channel images\n",
    "            slope_image_expanded = np.expand_dims(slope_image_normalized, axis=-1)\n",
    "            aspect_image_expanded = np.expand_dims(aspect_image_normalized, axis=-1)\n",
    "\n",
    "            # Concatenate all images\n",
    "            x_train = np.concatenate([\n",
    "                optic_image_normalized,\n",
    "                sar_image_normalized,\n",
    "                fvs_image_normalized,\n",
    "                slope_image_expanded,\n",
    "                aspect_image_expanded,\n",
    "                landsat8_image_normalized\n",
    "            ], axis=-1)\n",
    "\n",
    "            # Store the concatenated data\n",
    "            x_train_list.append(x_train)\n",
    "            agb_values_list.append(agb_map)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patch {idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Combine all patches into single arrays\n",
    "    x_data = np.stack(x_train_list)\n",
    "    agb_values = np.stack(agb_values_list)\n",
    "\n",
    "    # Normalize AGB values\n",
    "    agb_scaled = normalize(agb_values)\n",
    "    agb_normalized = np.expand_dims(agb_scaled, axis=-1)\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_data, agb_normalized, test_size=0.20, random_state=random_seed\n",
    "    )\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train, y_train, test_size=0.20, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    # Print dataset information\n",
    "    print('\\nDataset shapes:')\n",
    "    print('x_train:', x_train.shape)\n",
    "    print('y_train:', y_train.shape)\n",
    "    print('x_val:', x_val.shape)\n",
    "    print('y_val:', y_val.shape)\n",
    "    print('x_test:', x_test.shape)\n",
    "    print('y_test:', y_test.shape)\n",
    "\n",
    "    return {\n",
    "        'x_train': x_train,\n",
    "        'y_train': y_train,\n",
    "        'x_val': x_val,\n",
    "        'y_val': y_val,\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "        'input_shape': x_train.shape[1:]\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data with random sampling\n",
    "    data = load_remote_sensing_data(\n",
    "        desired_num_patches=400,\n",
    "        random_seed=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:09:58.426937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.448145: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.448275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.449174: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.449262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.449315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.490896: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.491014: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.491078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:09:58.491129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import custom_object_scope\n",
    "from keras.models import load_model\n",
    "from keras_unet_collection.transformer_layers import patch_extract, patch_embedding, patch_merging, patch_expanding, drop_path,WindowAttention, SwinTransformerBlock  # Assuming these are part of the custom layers\n",
    "\n",
    "# Include all custom objects that your model uses\n",
    "custom_objects = {\n",
    "    'patch_extract': patch_extract,\n",
    "    'patch_embedding': patch_embedding,\n",
    "    'patch_merging': patch_merging,\n",
    "    'patch_expanding': patch_expanding,\n",
    "    'drop_path': drop_path,\n",
    "    'WindowAttention': WindowAttention,\n",
    "    'SwinTransformerBlock': SwinTransformerBlock\n",
    "\n",
    "    # Add other custom layers or utilities as needed\n",
    "}\n",
    "\n",
    "# Load the model using the custom scope\n",
    "with custom_object_scope(custom_objects):\n",
    "    \n",
    "  model = load_model('result/Keras-Collecetion_Att_UNET/AGB_Att_UNET/31bands/model_for_AttentionUNET.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:19:27.534360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.554748: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.554886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.555777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.555908: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.555964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.608791: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.608917: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.608990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-11 22:19:27.609045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10037 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing permutation importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 22:19:28.657286: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8901\n",
      "2024-12-11 22:19:28.760161: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-12-11 22:19:30.085842: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "Analyzing features: 100%|██████████| 31/31 [01:04<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing occlusion sensitivity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing features: 100%|██████████| 31/31 [00:12<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gradient-based importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/80 [00:00<?, ?it/s]2024-12-11 22:20:52.496684: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-12-11 22:20:52.532821: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-12-11 22:20:52.587052: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-12-11 22:20:52.615611: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-12-11 22:20:52.777353: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-12-11 22:20:52.798776: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "Processing batches: 100%|██████████| 80/80 [00:03<00:00, 20.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# First, define the band names\n",
    "def create_band_names() -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates descriptive names for all 31 bands in the remote sensing dataset.\n",
    "    \"\"\"\n",
    "    # Optical bands (13 bands)\n",
    "    Sentinel2_bands = [\n",
    "        \"Sentinel2_Coastal-Aerosol\",           \n",
    "        \"Sentinel2_Blue\",          \n",
    "        \"Sentinel2_Green\",           \n",
    "        \"Sentinel2_Red\",      \n",
    "        \"Sentinel2_Red Edge 1\",      \n",
    "        \"Sentinel2_Red Edge 2\",      \n",
    "        \"Sentinel2_Red Edge 3\",           \n",
    "        \"Sentinel2_NIR\",          \n",
    "        \"Sentinel2_Red Edge 4\",         \n",
    "        \"Sentinel2_Water vapor\",         \n",
    "        \"Sentinel2_Cirrus\",          \n",
    "        \"Sentinel2_SWIR 1\",           \n",
    "        \"Sentinel2_SWIR 2\"           \n",
    "    ]\n",
    "    \n",
    "    # SAR bands (2 bands)\n",
    "    sar_bands = [\n",
    "        \"SAR_VH\",                # Vertical-Vertical polarization\n",
    "        \"SAR_VV\"                 # Vertical-Horizontal polarization\n",
    "    ]\n",
    "    \n",
    "    # Forest Vegetation Simulator (FVS) bands (7 bands)\n",
    "    fvs_bands = [\n",
    "        \"FVS_ffp\",        # Average tree height\n",
    "        \"FVS_sday\",        # Crown cover percentage\n",
    "        \"FVS_gsp\",         # Basal area\n",
    "        \"FVS_mmax\",           # Tree density\n",
    "        \"FVS_mmin\",           # Total biomass\n",
    "        \"FVS_mat\",      # Canopy height\n",
    "        \"FVS_map\"           # Stand age\n",
    "    ]\n",
    "    \n",
    "    # Topographic bands (2 bands)\n",
    "    topo_bands = [\n",
    "        \"Topo_Slope\",            # Terrain slope\n",
    "        \"Topo_Aspect\"            # Terrain aspect\n",
    "    ]\n",
    "    \n",
    "    # Landsat 8 bands (7 bands)\n",
    "    landsat_bands = [\n",
    "        \"Landsat8_Coastal\",      # Coastal/Aerosol band\n",
    "        \"Landsat8_Blue\",         # Blue band\n",
    "        \"Landsat8_Green\",        # Green band\n",
    "        \"Landsat8_Red\",          # Red band\n",
    "        \"Landsat8_NIR\",          # Near-infrared band\n",
    "        \"Landsat8_SWIR1\",        # Short-wave infrared 1\n",
    "        \"Landsat8_SWIR2\"         # Short-wave infrared 2\n",
    "    ]\n",
    "    \n",
    "    # Combine all bands\n",
    "    all_bands = (\n",
    "        Sentinel2_bands +\n",
    "        sar_bands +\n",
    "        fvs_bands +\n",
    "        topo_bands +\n",
    "        landsat_bands\n",
    "    )\n",
    "    \n",
    "    assert len(all_bands) == 31, f\"Expected 31 bands, got {len(all_bands)}\"\n",
    "    return all_bands\n",
    "\n",
    "# Wrapper for TensorFlow models\n",
    "class TFModelWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wrapper for TensorFlow models to make them scikit-learn compatible.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array(self.model.predict(X, verbose=0))\n",
    "\n",
    "# Main Feature Importance Analyzer class\n",
    "class FeatureImportanceAnalyzer:\n",
    "    \"\"\"Advanced feature importance analyzer for remote sensing models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, output_dir='feature_importance_results', batch_size=1):\n",
    "        self.model = model\n",
    "        self.feature_names = create_band_names()\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.results = {}\n",
    "        self.feature_groups = self._create_feature_groups()\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            filename=os.path.join(output_dir, 'analysis.log')\n",
    "        )\n",
    "    \n",
    "    def _create_feature_groups(self):\n",
    "        \"\"\"Creates groups of features based on their source.\"\"\"\n",
    "        groups = {\n",
    "            'Sentinel2': [],\n",
    "            'SAR': [],\n",
    "            'FVS': [],\n",
    "            'Topographic': [],\n",
    "            'Landsat8': []\n",
    "        }\n",
    "        \n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            if name.startswith('Sentinel2'):\n",
    "                groups['Sentinel2'].append(i)\n",
    "            elif name.startswith('SAR'):\n",
    "                groups['SAR'].append(i)\n",
    "            elif name.startswith('FVS'):\n",
    "                groups['FVS'].append(i)\n",
    "            elif name.startswith('Topo'):\n",
    "                groups['Topographic'].append(i)\n",
    "            elif name.startswith('Landsat8'):\n",
    "                groups['Landsat8'].append(i)\n",
    "                \n",
    "        return groups\n",
    "\n",
    "    def run_permutation_importance(self, X, y, n_repeats=5):\n",
    "        \"\"\"Compute permutation importance.\"\"\"\n",
    "        logging.info(\"Starting permutation importance analysis...\")\n",
    "        print(\"Computing permutation importance...\")\n",
    "        \n",
    "        wrapped_model = TFModelWrapper(self.model)\n",
    "        baseline_pred = wrapped_model.predict(X)\n",
    "        baseline_score = np.mean((baseline_pred - y) ** 2)\n",
    "        \n",
    "        importances = []\n",
    "        importance_std = []\n",
    "        \n",
    "        for feature_idx in tqdm(range(X.shape[-1]), desc=\"Analyzing features\"):\n",
    "            feature_importances = []\n",
    "            for _ in range(n_repeats):\n",
    "                X_permuted = X.copy()\n",
    "                permuted_idx = np.random.permutation(len(X))\n",
    "                X_permuted[..., feature_idx] = X_permuted[permuted_idx, ..., feature_idx]\n",
    "                \n",
    "                permuted_pred = wrapped_model.predict(X_permuted)\n",
    "                permuted_score = np.mean((permuted_pred - y) ** 2)\n",
    "                importance = permuted_score - baseline_score\n",
    "                feature_importances.append(importance)\n",
    "            \n",
    "            importances.append(np.mean(feature_importances))\n",
    "            importance_std.append(np.std(feature_importances))\n",
    "        \n",
    "        self.results['permutation'] = {\n",
    "            'mean': np.array(importances),\n",
    "            'std': np.array(importance_std)\n",
    "        }\n",
    "        return self.results['permutation']\n",
    "\n",
    "    def run_occlusion_sensitivity(self, X, patch_size=128):\n",
    "        \"\"\"Compute occlusion sensitivity.\"\"\"\n",
    "        logging.info(\"Starting occlusion sensitivity analysis...\")\n",
    "        print(\"Computing occlusion sensitivity...\")\n",
    "        \n",
    "        importances = np.zeros(len(self.feature_names))\n",
    "        importance_std = np.zeros(len(self.feature_names))\n",
    "        baseline = np.array(self.model.predict(X, verbose=0))\n",
    "        \n",
    "        for i in tqdm(range(X.shape[-1]), desc=\"Analyzing features\"):\n",
    "            impacts = []\n",
    "            for start_row in range(0, X.shape[1], patch_size):\n",
    "                for start_col in range(0, X.shape[2], patch_size):\n",
    "                    X_occluded = X.copy()\n",
    "                    end_row = min(start_row + patch_size, X.shape[1])\n",
    "                    end_col = min(start_col + patch_size, X.shape[2])\n",
    "                    X_occluded[:, start_row:end_row, start_col:end_col, i] = 0\n",
    "                    \n",
    "                    prediction = np.array(self.model.predict(X_occluded, verbose=0))\n",
    "                    impact = np.mean(np.abs(baseline - prediction))\n",
    "                    impacts.append(impact)\n",
    "            \n",
    "            importances[i] = np.mean(impacts)\n",
    "            importance_std[i] = np.std(impacts)\n",
    "        \n",
    "        self.results['occlusion'] = {\n",
    "            'mean': importances,\n",
    "            'std': importance_std\n",
    "        }\n",
    "        return self.results['occlusion']\n",
    "\n",
    "    def run_gradient_importance(self, X):\n",
    "        \"\"\"Compute gradient-based importance.\"\"\"\n",
    "        logging.info(\"Starting gradient-based importance analysis...\")\n",
    "        print(\"Computing gradient-based importance...\")\n",
    "        \n",
    "        importances = []\n",
    "        n_batches = int(np.ceil(len(X) / self.batch_size))\n",
    "        \n",
    "        for batch_idx in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, len(X))\n",
    "            X_batch = X[start_idx:end_idx]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                X_tf = tf.convert_to_tensor(X_batch, dtype=tf.float32)\n",
    "                tape.watch(X_tf)\n",
    "                predictions = self.model(X_tf)\n",
    "            \n",
    "            gradients = tape.gradient(predictions, X_tf)\n",
    "            batch_importances = np.mean(np.abs(gradients.numpy()), axis=(0, 1, 2))\n",
    "            importances.append(batch_importances)\n",
    "        \n",
    "        importances = np.array(importances)\n",
    "        self.results['gradient'] = {\n",
    "            'mean': np.mean(importances, axis=0),\n",
    "            'std': np.std(importances, axis=0)\n",
    "        }\n",
    "        return self.results['gradient']\n",
    "\n",
    "    def plot_feature_importance(self, method, grouped=False, figsize=(15, 10), color_palette='viridis'):\n",
    "        \"\"\"Plot feature importance results.\"\"\"\n",
    "        if method not in self.results:\n",
    "            raise ValueError(f\"Results for method '{method}' not found. Run analysis first.\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        if grouped:\n",
    "            grouped_mean = {group: np.mean(self.results[method]['mean'][indices]) \n",
    "                          for group, indices in self.feature_groups.items()}\n",
    "            grouped_std = {group: np.mean(self.results[method]['std'][indices]) \n",
    "                         for group, indices in self.feature_groups.items()}\n",
    "            \n",
    "            groups = list(grouped_mean.keys())\n",
    "            means = list(grouped_mean.values())\n",
    "            stds = list(grouped_std.values())\n",
    "            \n",
    "            colors = sns.color_palette(color_palette, n_colors=len(groups))\n",
    "            plt.bar(range(len(groups)), means, yerr=stds, capsize=5, color=colors)\n",
    "            plt.xticks(range(len(groups)), groups, rotation=45, ha='right')\n",
    "        else:\n",
    "            colors = sns.color_palette(color_palette, n_colors=len(self.feature_names))\n",
    "            plt.bar(range(len(self.feature_names)),\n",
    "                   self.results[method]['mean'],\n",
    "                   yerr=self.results[method]['std'],\n",
    "                   capsize=5,\n",
    "                   color=colors)\n",
    "            plt.xticks(range(len(self.feature_names)),\n",
    "                      self.feature_names,\n",
    "                      rotation=90,\n",
    "                      ha='right')\n",
    "        \n",
    "        plt.title(f\"{method.capitalize()} Feature Importance Analysis\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(\n",
    "            os.path.join(self.output_dir, f'{method}_importance{\"_grouped\" if grouped else \"\"}.png'),\n",
    "            dpi=300,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save analysis results to CSV.\"\"\"\n",
    "        results_dict = {\n",
    "            'Feature': self.feature_names,\n",
    "            'Category': [name.split('_')[0] for name in self.feature_names]\n",
    "        }\n",
    "        \n",
    "        for method, data in self.results.items():\n",
    "            results_dict[f\"{method}_importance\"] = data['mean']\n",
    "            results_dict[f\"{method}_std\"] = data['std']\n",
    "        \n",
    "        df = pd.DataFrame(results_dict)\n",
    "        \n",
    "        # Add summary statistics by category\n",
    "        summary_df = df.groupby('Category').agg({\n",
    "            col: ['mean', 'std'] for col in df.columns \n",
    "            if col.endswith('_importance')\n",
    "        })\n",
    "        \n",
    "        # Save both detailed and summary results\n",
    "        df.to_csv(os.path.join(self.output_dir, 'feature_importance_detailed.csv'), index=False)\n",
    "        summary_df.to_csv(os.path.join(self.output_dir, 'feature_importance_summary.csv'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "def analyze_feature_importance(model, X_test, y_test):\n",
    "    \"\"\"Main function to run all feature importance analyses.\"\"\"\n",
    "    # Create analyzer instance\n",
    "    analyzer = FeatureImportanceAnalyzer(model)\n",
    "    \n",
    "    # Run all analyses\n",
    "    analyzer.run_permutation_importance(X_test, y_test)\n",
    "    analyzer.run_occlusion_sensitivity(X_test)\n",
    "    analyzer.run_gradient_importance(X_test)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    for method in ['permutation', 'occlusion', 'gradient']:\n",
    "        analyzer.plot_feature_importance(method, grouped=False)\n",
    "        analyzer.plot_feature_importance(method, grouped=True)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = analyzer.save_results()\n",
    "    return analyzer, results_df\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "# Example usage\n",
    "    # Load your model and data\n",
    "    from keras.utils import custom_object_scope\n",
    "    from keras.models import load_model\n",
    "    from keras_unet_collection.transformer_layers import (\n",
    "        patch_extract, patch_embedding, patch_merging, patch_expanding,\n",
    "        drop_path, WindowAttention, SwinTransformerBlock\n",
    "    )\n",
    "    \n",
    "    # Custom objects dictionary also for Transformer models\n",
    "    custom_objects = {\n",
    "        'patch_extract': patch_extract,\n",
    "        'patch_embedding': patch_embedding,\n",
    "        'patch_merging': patch_merging,\n",
    "        'patch_expanding': patch_expanding,\n",
    "        'drop_path': drop_path,\n",
    "        'WindowAttention': WindowAttention,\n",
    "        'SwinTransformerBlock': SwinTransformerBlock\n",
    "    }\n",
    "    \n",
    "    # Load the model\n",
    "    with custom_object_scope(custom_objects):\n",
    "        model = load_model('Model.h5')\n",
    "    X_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    analyzer, results = analyze_feature_importance(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
